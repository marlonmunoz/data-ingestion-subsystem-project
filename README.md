# ðŸ§± Data Ingestion Subsystem

## Overview

A production-ready ETL pipeline that extracts real estate data from CSV, validates and cleans it, then loads it into PostgreSQL staging tables. Built with Python, pandas, and psycopg2, featuring comprehensive logging and 93% test coverage.

---

## ðŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run the pipeline
python src/main.py
```

---

## ðŸ“Š Pipeline Flow

```
CSV Data â†’ Extract â†’ Clean â†’ Validate â†’ Load â†’ PostgreSQL
                                        â†“
                                   Rejects Table
```

### 1ï¸âƒ£ **Extract** (`csv_read.py`)
- Reads CSV files using pandas
- Loads 9,129 real estate property records
- Logs data shape and column information

### 2ï¸âƒ£ **Clean** (`clean.py`)
- Renames columns to snake_case
- Strips whitespace from string fields
- Handles missing values (fills or drops)
- Converts date formats

### 3ï¸âƒ£ **Validate** (`validate.py`, `rules.py`)
- Validates required fields (location_id, property_type, etc.)
- Checks numeric ranges (parking_spaces â‰¥ 0)
- Removes duplicate records by primary key
- Tracks rejection reasons for invalid data

### 4ï¸âƒ£ **Load** (`load.py`)
- Connects to PostgreSQL database
- Creates staging tables (`stg_real_estate`, `stg_rejects`)
- Performs UPSERT operations (INSERT with conflict handling)
- Batch processing for performance (1000 rows/batch)
- Logs rejected records as JSONB with error details

---

## ðŸ—„ï¸ Database Schema

| Table | Purpose |
|-------|---------|
| **stg_real_estate** | Valid property records (15 columns) |
| **stg_rejects** | Invalid records with rejection reasons |

---

## âœ… Testing

```bash
# Run all tests
pytest

# Run with coverage report
pytest --cov=src --cov-report=term-missing
```

**Test Results:**
- 48 tests passing
- 93% code coverage
- 100% coverage on core modules (clean, validate, rules, load, main)

---

## ðŸ“ Project Structure

```
ingestion/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sources.json          # Pipeline configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ real_estate.csv       # Source data (9,129 records)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ utils.py              # Logger configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py               # ETL orchestrator
â”‚   â”œâ”€â”€ clean.py              # Data cleaning functions
â”‚   â”œâ”€â”€ validate.py           # Validation rules
â”‚   â”œâ”€â”€ rules.py              # Validation pipeline
â”‚   â”œâ”€â”€ load.py               # PostgreSQL loader
â”‚   â”œâ”€â”€ config.py             # Config loader
â”‚   â””â”€â”€ readers/
â”‚       â””â”€â”€ csv_read.py       # CSV reader
â””â”€â”€ tests/
    â”œâ”€â”€ test_clean.py
    â”œâ”€â”€ test_validate.py
    â”œâ”€â”€ test_rules.py
    â”œâ”€â”€ test_load.py
    â””â”€â”€ test_main.py
```

---

## ðŸ”§ Configuration

Edit `config/sources.json` to customize:
- Database connection URL
- Source file paths
- Target table names

---

## ðŸ“ Logging

All pipeline operations are logged to:
- **Console**: Real-time progress updates
- **File**: `logs/pipeline_YYYYMMDD.log`

Log levels: INFO (success), ERROR (failures)

---

## ðŸŽ¯ Key Features

âœ… Modular ETL design  
âœ… Comprehensive error handling  
âœ… UPSERT for idempotent loads  
âœ… Rejection tracking with detailed reasons  
âœ… Batch processing for large datasets  
âœ… 93% test coverage  
âœ… Production-ready logging  

---

## ðŸ› ï¸ Technologies

- **Python 3.11.9**
- **pandas** - Data manipulation
- **psycopg2** - PostgreSQL adapter
- **pytest** - Testing framework
- **PostgreSQL** - Data warehouse

---

## âš™ï¸ Standard Functional Scope

The application will:

1. Be written in **Python** and connect to a **PostgreSQL** database.
2. Use **configuration files** (YAML or JSON) to define data sources, schemas, and rules.
3. Handle errors gracefully, continuing with valid records even if some fail.
4. Allow new data sources to be added without major code changes.
5. Log every step of the ingestion for auditing and debugging.

---

## âœ… Definition of Done

The **Data Ingestion Subsystem** will be considered complete when:

* It can successfully read and load data from a source (CSV, API, JSON, PDF etc) files into PostgreSQL.
* At least **80% of the code is covered by PyTest**.
* Database connections are safely opened and closed.
* (optional) All rejected rows are logged with reasons in `stg_rejects`.
* The final demo and code repository are available for review.

---

## ðŸ§± Non-Functional Expectations

* Design must be **simple, modular, and maintainable**.
* Code follows **PEP 8** and standard naming conventions.
* Use **parameterized queries** to prevent SQL injection.
* Configuration and credentials stored securely (e.g., `.env`).
* All code and logs version-controlled with Git.

---

## ðŸ§­ System Architecture Overview

**Data Flow:**

```
Source (CSV/JSON/API)
    â†“
Reader Layer (pandas)
    â†“
Validation & Cleaning
    â†“
Duplicate Removal
    â†“
PostgreSQL Loader (UPSERT)
    â†“
Rejects Table + Structured Logs
```

**Core Layers:**

| Layer         | Responsibility                                                |
| ------------- | ------------------------------------------------------------- |
| **Reader**    | Reads CSV, JSON, or API data into memory (pandas DataFrame).  |
| **Validator** | Ensures schema conformity, checks nulls, enforces data rules. |
| **Cleaner**   | Fixes or drops invalid data, normalizes column names.         |
| **Loader**    | Loads valid data into staging tables using batch UPSERT.      |
| **Logger**    | Captures structured logs and rejects with detailed reasons.   |

---

## ðŸ“ Optional, but Recommended Folder Structure

```
ingestion/
  src/
    config.py
    readers/ (csv_reader.py, json_reader.py, api_reader.py)
    validate.py
    clean.py
    load.py
    rules.py
    main.py
  config/
    sources.yml
  data/
    customers.csv
    sales.json
  tests/
    test_validate.py
    test_load.py
  requirements.txt
  README.md
```

---

## âš™ï¸ Example Configuration File (YAML)

This configuration file defines all ingestion sources and validation rules.

```yaml
# config/sources.yml
defaults:
  db_url: postgresql+psycopg://user:pass@localhost:5432/ingest_db
  batch_size: 5000
  on_conflict: upsert           # options: append | upsert | fail

sources:
  - name: customers_csv
    type: csv
    path: data/customers.csv
    target_table: stg_customers
    pk: [customer_id]
    schema:
      customer_id: int
      first_name: str
      last_name: str
      email: str
      created_at: datetime
    rules:
      - rule: "email LIKE '%@%'"
      - rule: "created_at NOT NULL"
      - rule: "len(first_name) > 0"

  - name: sales_json
    type: json
    path: data/sales.json
    target_table: stg_sales
    pk: [sale_id]
    schema:
      sale_id: int
      customer_id: int
      amount: float
      currency: str
      ts: datetime
    rules:
      - rule: "amount >= 0"
      - rule: "currency IN ('USD','EUR','CAD')"
```

---

## ðŸ—ƒï¸ Example Database Design

### Staging Tables

```sql
CREATE TABLE IF NOT EXISTS stg_customers (
  customer_id   BIGINT PRIMARY KEY,
  first_name    TEXT NOT NULL,
  last_name     TEXT NOT NULL,
  email         TEXT,
  created_at    TIMESTAMP,
  _loaded_at    TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS stg_sales (
  sale_id       BIGINT PRIMARY KEY,
  customer_id   BIGINT NOT NULL,
  amount        NUMERIC(12,2) NOT NULL CHECK (amount >= 0),
  currency      TEXT NOT NULL,
  ts            TIMESTAMP NOT NULL,
  _loaded_at    TIMESTAMP NOT NULL DEFAULT NOW(),
  FOREIGN KEY (customer_id) REFERENCES stg_customers(customer_id)
);

CREATE TABLE IF NOT EXISTS stg_rejects (
  source_name   TEXT NOT NULL,
  raw_payload   JSONB NOT NULL,
  reason        TEXT NOT NULL,
  rejected_at   TIMESTAMP NOT NULL DEFAULT NOW()
);
```

---

## ðŸ”„ Loading Strategy

To ensure **idempotency**, the loader uses an **UPSERT** pattern:
* The upsert pattern is a database operation that combines insert and update functionality, allowing a new record to be inserted if it does not exist, or an existing record to be updated if it does.

```sql
INSERT INTO stg_sales (sale_id, customer_id, amount, currency, ts)
VALUES (:sale_id, :customer_id, :amount, :currency, :ts)
ON CONFLICT (sale_id) DO UPDATE
SET customer_id = EXCLUDED.customer_id,
    amount      = EXCLUDED.amount,
    currency    = EXCLUDED.currency,
    ts          = EXCLUDED.ts;
```

---

## ðŸ§¹ Validation & Cleaning Rules

| Step                | Action                                                      |
| ------------------- | ----------------------------------------------------------- |
| **Type Casting**    | Convert columns to schema types; drop rows that canâ€™t cast. |
| **Required Fields** | Drop rows missing critical values.                          |
| **Domain Checks**   | Ensure numeric and categorical rules (e.g., amount â‰¥ 0).    |
| **Deduplication**   | Drop duplicates by PK before loading.                       |
| **Reject Logging**  | Store invalid rows in `stg_rejects` for inspection.         |

---

## ðŸªµ Structured Logging Examples

```
INFO ingest.start source=customers_csv rows=12034 path=data/customers.csv
INFO ingest.cleaned source=customers_csv valid=11890 rejected=144
INFO ingest.load source=customers_csv inserted=11750 updated=140 duration=3.2s
INFO ingest.end source=customers_csv status=success
```

Each run produces a concise summary of:

* total input rows
* valid vs rejected counts
* inserted vs updated records
* run duration and status

---

## ðŸ§ª Testing Strategy (PyTest)

| Type                  | Focus                                                             |
| --------------------- | ----------------------------------------------------------------- |
| **Unit Tests**        | Type casting, rule evaluation, duplicate removal, SQL generation. |
| **Integration Tests** | Load sample CSV/JSON to Postgres and verify counts.               |
| **Property Tests**    | Validate numerical domains (e.g., all amounts â‰¥ 0).               |

> Use Docker or local Postgres for integration testing.

Target: **â‰¥ 80% coverage**

---

## ðŸ§  Example Pseudocode Flow

```python
def run_source(cfg):
    df = read_input(cfg)                 # csv/json/api â†’ DataFrame
    df = normalize_columns(df)           # trim/strip/lower headers
    df, rejects = apply_schema_casts(df, cfg.schema)
    df = enforce_required(df, cfg.schema)
    df, new_rejects = apply_rules(df, cfg.rules)
    rejects += new_rejects
    df = drop_duplicates(df, pk=cfg.pk)
    load_upsert(df, table=cfg.target_table, pk=cfg.pk, batch=cfg.batch_size)
    write_rejects(rejects, source_name=cfg.name)
    log_summary(cfg.name, len(df), len(rejects))
```

---

## ðŸ§© Definition of Done (Final)

* âœ… CSV and JSON data ingested into **PostgreSQL staging tables**.
* âœ… All **rejected rows captured** with reason and payload.
* âœ… Configuration-driven source definitions.
* âœ… Structured logging with run summary.
* âœ… **80%+ PyTest coverage**.
* âœ… Proper connection management.
* âœ… Clean modular design and naming conventions.
* âœ… Version-controlled repository with **README** and demo notebook.

---

## ðŸš€ Stretch Goals (Optional Enhancements)

| Area               | Feature                                                                                                 |
| ------------------ | ------------------------------------------------------------------------------------------------------- |
| **Ingestion**      | Implement incremental loads using timestamp or surrogate key watermark.                                 |
| **Validation**     | Add schema validation with `pydantic` or data quality checks via `great_expectations`.                  |
| **Storage**        | Partition or cluster Postgres tables for large-scale analytics.                                         |
| **Deployment**     | Use **Docker Compose** for local Postgres + app environment.                                            |
| **Monitoring**     | Track load metrics (rows/sec, rejects/sec) and log to a dashboard.                                      |
| **Security**       | Move DB credentials and API keys to `.env` or secret manager.                                           |
| **Analytics**      | Create summary views or materialized tables for downstream use.                                         |
| **AI Integration** | Build a simple **model pipeline** (e.g., train a regression or classifier on cleaned data).             |
| **Feature Store**  | Extract and store **ML features** in Postgres for reuse in predictions.                                 |
| **Embedding Demo** | Generate vector embeddings from text columns using `sentence-transformers` or `OpenAI Embeddings` API.  |
| **RAG Experiment** | Add a **Retrieval-Augmented Generation** notebook that indexes sample data and queries it via LLM.      |
| **Auto Insights**  | Use a lightweight LLM (like `llama-cpp-python` or `OpenAI API`) to summarize or describe data patterns. |

---

## ðŸ§­ Summary

The **Data Ingestion Subsystem** is a foundational, end-to-end data engineering project.
It simulates real-world ingestion pipelines â€” combining:

* **Python scripting** for data reading and validation,
* **SQL modeling** for data integrity and staging,
* **Logging and testing** for maintainability and observability.

By completing this project, we will gain a full picture of how **raw data becomes trustworthy, query-ready data**, setting them up for real data engineering, analytics, and cloud-based ELT systems.


## Activate Virtual .venv
`source .venv/bin/activate`

## Deactivate
`deactivate`


## ETL STEPS FLOW

- Step 1: config.py   âœ…............. â†’ Load configuration 
- Step 2: csv_read.py âœ…............. â†’ Read CSV into DataFrame
- Step 3: clean.py    âœ…............. â†’ Clean columns and values (T)
- Step 4: validate.py âœ…............. â†’ Validate data quality (T)
- Step 5: rules.py    âœ…............. â†’ Orchestrate all validations
- Step 6: load.py     âœ…............ â†’ Connected and load valid data into psql (T)
- Step 7: main.py     âœ…............. â†’ Orchetrator that ties everything together
- Step 8: testing 



## PYTEST commands

#### Run all tests in current directory and subdirectories
`pytest`

#### Run tests in a specific file
`pytest test_file.py`

#### Run a specific test function
`pytest test_file.py::test_function_name`

#### Run tests in a specific directory
`pytest tests/`

#### Run with verbose output
`pytest -v`

#### Run with output from print statements
`pytest -s`

#### Run and show coverage report
`pytest --cov`

#### Run and stop at first failure
`pytest -x`

---

## ðŸ”§ Troubleshooting

### Connection Pool Timeout Error

If you see this error:
```
pgsql: Failed to expand node: Timeout getting connection from pool. 
pool size: 10, max: 10, active tx: 0
```

**Cause:** Too many idle connections from DBeaver, VS Code PostgreSQL extension, or other database clients are holding connection pool slots.

**Solution:** Run the fix script:
```bash
python scripts/fix_connection_pool.py
```

This will terminate idle connections and free up the pool. 

**Prevention:**
- Close DBeaver connections when not in use
- Disconnect VS Code PostgreSQL extension when done querying
- Ensure your code properly closes database connections with `conn.close()`