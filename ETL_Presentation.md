# Real Estate Data Ingestion System
## A Production-Ready ETL Pipeline

**Project:** Data Ingestion Subsystem  
**Tech Stack:** Python | PostgreSQL | Pandas | Pytest  
**Quality:** 93% Test Coverage | 56 Passing Tests

---

# Project Overview

## Business Problem
- Process **9,129 real estate property records** from CSV files
- Ensure data quality through comprehensive validation
- Load clean data into PostgreSQL for analytics

## Solution Delivered
- âœ… Automated ETL pipeline with comprehensive error handling
- âœ… Separate valid and invalid records with full traceability
- âœ… Complete audit trail via logging and reject tables
- âœ… 93% test coverage for production reliability

---

# Architecture & Tech Stack

## Core Technologies
- **Python 3.11** - Pipeline orchestration & business logic
- **Pandas** - Data manipulation & transformation
- **PostgreSQL** - Enterprise data warehouse
- **Psycopg2** - Database connectivity & connection pooling
- **Pytest** - Comprehensive unit testing framework

## Key Features
- ğŸ“¦ Modular design (16 Python files)
- ğŸ“Š JSONB storage for flexible rejected records
- âš¡ Batch processing (1000 rows/batch)
- ğŸ”’ Transaction management & rollback support

---

# ETL Pipeline Flow & Medallion Architecture

## Data Quality Layers

```
ğŸŸ¤ BRONZE (Raw)              ğŸ¥ˆ SILVER (Validated)           ğŸ¥‡ GOLD (Business-Ready)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ real_estate.csv  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ stg_real_estate  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Future Layer    â”‚
â”‚ (9,129 records)  â”‚         â”‚ (4,110 valid)    â”‚           â”‚  â€¢ Aggregations  â”‚
â”‚                  â”‚         â”‚                  â”‚           â”‚  â€¢ Dimensions    â”‚
â”‚ â€¢ Raw columns    â”‚         â”‚ â€¢ Clean names    â”‚           â”‚  â€¢ Fact tables   â”‚
â”‚ â€¢ Invalid data   â”‚         â”‚ â€¢ Validated      â”‚           â”‚  â€¢ Business KPIs â”‚
â”‚ â€¢ Duplicates     â”‚         â”‚ â€¢ Deduplicated   â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   stg_rejects    â”‚
â”‚ (5,019 rejected) â”‚
â”‚ â€¢ JSONB format   â”‚
â”‚ â€¢ Audit trail    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## Pipeline Steps

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        EXTRACT              â”‚
â”‚   CSV Reader Module         â”‚
â”‚   (9,129 records loaded)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       TRANSFORM             â”‚
â”‚  â€¢ Rename columns           â”‚
â”‚  â€¢ Strip whitespace         â”‚
â”‚  â€¢ Handle missing values    â”‚
â”‚  â€¢ Convert date formats     â”‚
â”‚  â€¢ Validate business rules  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LOAD                â”‚
â”‚  PostgreSQL UPSERT          â”‚
â”‚  (Conflict handling)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ stg_real_estate  â”‚   stg_rejects    â”‚
â”‚ (4,110 valid)    â”‚  (5,019 invalid) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Data Cleaning Module

## Transformation Functions

### 1. Column Renaming (snake_case standardization)
- `"data.date"` â†’ `data_date`
- `"location.address.city"` â†’ `city`
- `"data.parking spaces"` â†’ `parking_spaces`

### 2. Whitespace Removal
- Strip leading/trailing spaces from all text fields
- Prevents duplicate entries from spacing variations

### 3. Missing Value Handling
- Convert placeholder values (`"0"`) to proper NULL
- Explicit handling for data quality

### 4. Date Conversion
- Parse string dates to timestamp format
- Error handling with coercion for invalid dates

**Result:** Clean, standardized, database-ready records

---

# Data Validation & Quality

## Comprehensive Validation Rules

| Rule Type | Validation Logic | Action Taken |
|-----------|-----------------|--------------|
| **Required Fields** | location_id, city, state must exist | âŒ Reject if NULL/empty |
| **NULL Validation** | data_date, property_type, zip_code, ownership_type, address_line1 | âŒ Reject if NULL |
| **Numeric Ranges** | parking_spaces â‰¥ 0 | âŒ Reject if negative |
| **Duplicates** | Check primary key (location_id) | âš ï¸ Keep first, reject rest |

## Current Pipeline Results
- âœ… **4,110 valid records (45%)** â†’ Loaded to `stg_real_estate`
- âŒ **5,019 rejected records (55%)** â†’ Logged to `stg_rejects`
- ğŸ“Š Primary rejection reason: NULL `data_date` values in source data

---

# Database Schema Design

## ğŸŸ¤ Bronze Layer: Raw Data Preservation

**Table: stg_rejects** (Invalid Records)
- Stores rejected records in original format
- `raw_data` (JSONB) - Complete raw record with original column names
- Maintains audit trail of all failed validations
- **Example:** `{"data.date": "0", "location.id": "CT3316"}`

## ğŸ¥ˆ Silver Layer: Validated & Cleaned

**Table: stg_real_estate** (Valid Records)
- 15 standardized columns with clean snake_case names
- `location_id` (PK), `city`, `state`, `data_date`, `parking_spaces`, etc.
- UPSERT logic: `INSERT ... ON CONFLICT DO UPDATE`
- Indexed for fast analytical queries

**View: v_rejects_expanded**
- Expands Bronze JSONB into Silver-quality columns
- Bridges Bronze â†’ Silver for rejected data analysis

## ğŸ¥‡ Gold Layer: Future Enhancement
- Aggregated dimension tables (dim_location, dim_property)
- Business-specific KPIs and metrics
- Denormalized fact tables for reporting

## Why JSONB for Bronze?
âœ… **Preserves raw format** - Historical accuracy  
âœ… **Flexible schema** - Handle any source changes  
âœ… **Queryable** - PostgreSQL JSONB operators  
âœ… **Indexable** - GIN indexes for performance

---

# Error Handling & Logging

## Comprehensive Logging System
**File Output:** `logs/pipeline.log`  
**Console Output:** Real-time progress updates  
**Format:** `2025-12-10 09:45:32 - etl_pipeline - INFO - Message`

## Three-Tier Error Strategy

### 1. Validation Errors
- Logged to `stg_rejects` with specific rejection reason
- Full record preserved in JSONB for analysis
- Enables data quality reporting

### 2. Database Errors
- Automatic transaction rollback
- Preserves data integrity (ACID compliance)
- Detailed error messages logged

### 3. Connection Errors
- Graceful pipeline failure
- Clear error messages for troubleshooting
- No partial data commits

## Complete Audit Trail
âœ… Every rejected record tracked with reason & timestamp  
âœ… Full data preservation in JSONB format  
âœ… Pipeline execution metrics logged

---

# Testing & Quality Assurance

## Test Coverage: 93% (333 lines tested, 22 untested)

| Module | Lines | Coverage | Tests | Status |
|--------|-------|----------|-------|--------|
| **clean.py** | 30 | 100% âœ… | 8 | All passing |
| **validate.py** | 68 | 100% âœ… | 12 | All passing |
| **rules.py** | 36 | 97% âœ… | 7 | All passing |
| **load.py** | 83 | 100% âœ… | 17 | All passing |
| **main.py** | 59 | 100% âœ… | 4 | All passing |
| **utils/** | 19 | 100% âœ… | - | Integrated |

## Test Categories
- âœ… **Unit Tests** - Individual function validation
- âœ… **Integration Tests** - Database operations & transactions
- âœ… **Edge Cases** - Empty DataFrames, NULL values, duplicates
- âœ… **Error Scenarios** - Connection failures, rollbacks, invalid data

**Result:** 56/56 tests passing âœ… (0.68 seconds execution time)

---

# Results & Key Takeaways

## Pipeline Performance Metrics
- âš¡ **Processing Speed:** 9,129 records in seconds
- ğŸ“¦ **Scalability:** Batch processing (1000 rows/batch)
- ğŸ”’ **Data Integrity:** Zero data loss (all records tracked)
- ğŸ›¡ï¸ **Reliability:** Transaction safety with rollback support

## Key Achievements

### 1. Modular Design
- Reusable components for future data sources (API, JSON readers)
- Clear separation of concerns (Extract, Transform, Load)

### 2. Data Quality Enforcement
- Strict validation prevents bad data from polluting warehouse
- 45% success rate reflects **actual source data quality**

### 3. Full Observability
- Complete logging & audit trail for compliance
- JSONB reject storage enables root cause analysis

### 4. Production-Ready Reliability
- 93% test coverage ensures stability
- ACID transactions protect data integrity

## Next Steps & Roadmap
1. ğŸ“¡ Add API and JSON readers for additional sources
2. ğŸ”„ Implement incremental/delta loads (process only changes)
3. ğŸ“Š Build data quality metrics dashboard
4. â° Schedule automated pipeline runs (cron/Airflow)
5. ğŸš€ Deploy to production environment with monitoring

---

# Questions?

## Contact Information
**Project:** Data Ingestion Subsystem  
**Repository:** github.com/marlonmunoz/data-ingestion-subsystem-project

## Key Metrics Summary
- ğŸ“Š **9,129** total records processed
- âœ… **4,110** valid records (45%)
- âŒ **5,019** rejected records (55%)
- ğŸ§ª **56** passing tests
- ğŸ“ˆ **93%** code coverage
- âš¡ **< 1 second** test execution time

**Thank you!**
